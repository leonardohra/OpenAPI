swagger: '2.0'
schemes:
  - https
host: vision.googleapis.com
basePath: /
info:
  contact:
    name: Google
    url: 'https://google.com'
  description: 'Integrates Google Vision features, including image labeling, face, logo, and landmark detection, optical character recognition (OCR), and detection of explicit content, into applications.'
  title: Cloud Vision
  version: v1
  x-apiClientRegistration:
    url: 'https://console.developers.google.com'
  x-apisguru-categories:
    - machine_learning
    - analytics
  x-logo:
    url: 'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png'
  x-origin:
    - converter:
        url: 'https://github.com/lucybot/api-spec-converter'
        version: 2.6.2
      format: google
      url: 'https://vision.googleapis.com/$discovery/rest?version=v1'
      version: v1
  x-preferred: true
  x-providerName: googleapis.com
  x-serviceName: vision
externalDocs:
  url: 'https://cloud.google.com/vision/'
securityDefinitions:
  Oauth2:
    authorizationUrl: 'https://accounts.google.com/o/oauth2/auth'
    description: Oauth 2.0 authentication
    flow: implicit
    scopes:
      'https://www.googleapis.com/auth/cloud-platform': View and manage your data across Google Cloud Platform services
      'https://www.googleapis.com/auth/cloud-vision': Apply machine learning models to understand and label images
    type: oauth2
parameters:
  $.xgafv:
    description: V1 error format.
    enum:
      - '1'
      - '2'
    in: query
    name: $.xgafv
    type: string
  access_token:
    description: OAuth access token.
    in: query
    name: access_token
    type: string
  alt:
    default: json
    description: Data format for response.
    enum:
      - json
      - media
      - proto
    in: query
    name: alt
    type: string
  bearer_token:
    description: OAuth bearer token.
    in: query
    name: bearer_token
    type: string
  callback:
    description: JSONP
    in: query
    name: callback
    type: string
  fields:
    description: Selector specifying which fields to include in a partial response.
    in: query
    name: fields
    type: string
  key:
    description: 'API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.'
    in: query
    name: key
    type: string
  oauth_token:
    description: OAuth 2.0 token for the current user.
    in: query
    name: oauth_token
    type: string
  pp:
    default: true
    description: Pretty-print response.
    in: query
    name: pp
    type: boolean
  prettyPrint:
    default: true
    description: Returns response with indentations and line breaks.
    in: query
    name: prettyPrint
    type: boolean
  quotaUser:
    description: 'Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.'
    in: query
    name: quotaUser
    type: string
  uploadType:
    description: 'Legacy upload protocol for media (e.g. "media", "multipart").'
    in: query
    name: uploadType
    type: string
  upload_protocol:
    description: 'Upload protocol for media (e.g. "raw", "multipart").'
    in: query
    name: upload_protocol
    type: string
tags:
  - name: files
  - name: images
  - name: locations
  - name: operations
paths:
  '/v1/files:asyncBatchAnnotate':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: |-
        Run asynchronous image detection and annotation for a list of generic
        files, such as PDF files, which may contain multiple pages and multiple
        images per page. Progress and results can be retrieved through the
        `google.longrunning.Operations` interface.
        `Operation.metadata` contains `OperationMetadata` (metadata).
        `Operation.response` contains `AsyncBatchAnnotateFilesResponse` (results).
      operationId: vision.files.asyncBatchAnnotate
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/AsyncBatchAnnotateFilesRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-vision'
      tags:
        - files
  '/v1/images:annotate':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: Run image detection and annotation for a batch of images.
      operationId: vision.images.annotate
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/BatchAnnotateImagesRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/BatchAnnotateImagesResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-vision'
      tags:
        - images
  '/v1/{name}':
    delete:
      description: |-
        Deletes a long-running operation. This method indicates that the client is
        no longer interested in the operation result. It does not cancel the
        operation. If the server doesn't support this method, it returns
        `google.rpc.Code.UNIMPLEMENTED`.
      operationId: vision.operations.delete
      parameters:
        - description: The name of the operation resource to be deleted.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Empty'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-vision'
      tags:
        - operations
    get:
      description: |-
        Gets the latest state of a long-running operation.  Clients can use this
        method to poll the operation result at intervals as recommended by the API
        service.
      operationId: vision.locations.operations.get
      parameters:
        - description: The name of the operation resource.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-vision'
      tags:
        - locations
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
  '/v1/{name}:cancel':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: |-
        Starts asynchronous cancellation on a long-running operation.  The server
        makes a best effort to cancel the operation, but success is not
        guaranteed.  If the server doesn't support this method, it returns
        `google.rpc.Code.UNIMPLEMENTED`.  Clients can use
        Operations.GetOperation or
        other methods to check whether the cancellation succeeded or whether the
        operation completed despite cancellation. On successful cancellation,
        the operation is not deleted; instead, it becomes an operation with
        an Operation.error value with a google.rpc.Status.code of 1,
        corresponding to `Code.CANCELLED`.
      operationId: vision.operations.cancel
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/CancelOperationRequest'
        - description: The name of the operation resource to be cancelled.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Empty'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-vision'
      tags:
        - operations
definitions:
  AnnotateFileResponse:
    description: |-
      Response to a single file annotation request. A file may contain one or more
      images, which individually have their own responses.
    properties:
      inputConfig:
        $ref: '#/definitions/InputConfig'
        description: Information about the file for which this response is generated.
      responses:
        description: Individual responses to images found within the file.
        items:
          $ref: '#/definitions/AnnotateImageResponse'
        type: array
    type: object
  AnnotateImageRequest:
    description: |-
      Request for performing Google Cloud Vision API tasks over a user-provided
      image, with user-requested features.
    properties:
      features:
        description: Requested features.
        items:
          $ref: '#/definitions/Feature'
        type: array
      image:
        $ref: '#/definitions/Image'
        description: The image to be processed.
      imageContext:
        $ref: '#/definitions/ImageContext'
        description: Additional context that may accompany the image.
    type: object
  AnnotateImageResponse:
    description: Response to an image annotation request.
    properties:
      context:
        $ref: '#/definitions/ImageAnnotationContext'
        description: |-
          If present, contextual information is needed to understand where this image
          comes from.
      cropHintsAnnotation:
        $ref: '#/definitions/CropHintsAnnotation'
        description: 'If present, crop hints have completed successfully.'
      error:
        $ref: '#/definitions/Status'
        description: |-
          If set, represents the error message for the operation.
          Note that filled-in image annotations are guaranteed to be
          correct, even when `error` is set.
      faceAnnotations:
        description: 'If present, face detection has completed successfully.'
        items:
          $ref: '#/definitions/FaceAnnotation'
        type: array
      fullTextAnnotation:
        $ref: '#/definitions/TextAnnotation'
        description: |-
          If present, text (OCR) detection or document (OCR) text detection has
          completed successfully.
          This annotation provides the structural hierarchy for the OCR detected
          text.
      imagePropertiesAnnotation:
        $ref: '#/definitions/ImageProperties'
        description: 'If present, image properties were extracted successfully.'
      labelAnnotations:
        description: 'If present, label detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      landmarkAnnotations:
        description: 'If present, landmark detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      logoAnnotations:
        description: 'If present, logo detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      safeSearchAnnotation:
        $ref: '#/definitions/SafeSearchAnnotation'
        description: 'If present, safe-search annotation has completed successfully.'
      textAnnotations:
        description: 'If present, text (OCR) detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      webDetection:
        $ref: '#/definitions/WebDetection'
        description: 'If present, web detection has completed successfully.'
    type: object
  AsyncAnnotateFileRequest:
    description: An offline file annotation request.
    properties:
      features:
        description: Required. Requested features.
        items:
          $ref: '#/definitions/Feature'
        type: array
      imageContext:
        $ref: '#/definitions/ImageContext'
        description: Additional context that may accompany the image(s) in the file.
      inputConfig:
        $ref: '#/definitions/InputConfig'
        description: Required. Information about the input file.
      outputConfig:
        $ref: '#/definitions/OutputConfig'
        description: Required. The desired output location and metadata (e.g. format).
    type: object
  AsyncAnnotateFileResponse:
    description: The response for a single offline file annotation request.
    properties:
      outputConfig:
        $ref: '#/definitions/OutputConfig'
        description: The output location and metadata from AsyncAnnotateFileRequest.
    type: object
  AsyncBatchAnnotateFilesRequest:
    description: |-
      Multiple async file annotation requests are batched into a single service
      call.
    properties:
      requests:
        description: Individual async file annotation requests for this batch.
        items:
          $ref: '#/definitions/AsyncAnnotateFileRequest'
        type: array
    type: object
  AsyncBatchAnnotateFilesResponse:
    description: Response to an async batch file annotation request.
    properties:
      responses:
        description: |-
          The list of file annotation responses, one for each request in
          AsyncBatchAnnotateFilesRequest.
        items:
          $ref: '#/definitions/AsyncAnnotateFileResponse'
        type: array
    type: object
  BatchAnnotateImagesRequest:
    description: Multiple image annotation requests are batched into a single service call.
    properties:
      requests:
        description: Individual image annotation requests for this batch.
        items:
          $ref: '#/definitions/AnnotateImageRequest'
        type: array
    type: object
  BatchAnnotateImagesResponse:
    description: Response to a batch image annotation request.
    properties:
      responses:
        description: Individual responses to image annotation requests within the batch.
        items:
          $ref: '#/definitions/AnnotateImageResponse'
        type: array
    type: object
  Block:
    description: Logical element on the page.
    properties:
      blockType:
        description: 'Detected block type (text, image etc) for this block.'
        enum:
          - UNKNOWN
          - TEXT
          - TABLE
          - PICTURE
          - RULER
          - BARCODE
        type: string
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the block.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:

          * when the text is horizontal it might look like:

                  0----1
                  |    |
                  3----2

          * when it's rotated 180 degrees around the top-left corner it becomes:

                  2----3
                  |    |
                  1----0

            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results on the block. Range [0, 1].'
        format: float
        type: number
      paragraphs:
        description: List of paragraphs in this block (if this blocks is of type text).
        items:
          $ref: '#/definitions/Paragraph'
        type: array
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the block.
    type: object
  BoundingPoly:
    description: A bounding polygon for the detected image annotation.
    properties:
      normalizedVertices:
        description: The bounding polygon normalized vertices.
        items:
          $ref: '#/definitions/NormalizedVertex'
        type: array
      vertices:
        description: The bounding polygon vertices.
        items:
          $ref: '#/definitions/Vertex'
        type: array
    type: object
  CancelOperationRequest:
    description: The request message for Operations.CancelOperation.
    properties: {}
    type: object
  Color:
    description: |-
      Represents a color in the RGBA color space. This representation is designed
      for simplicity of conversion to/from color representations in various
      languages over compactness; for example, the fields of this representation
      can be trivially provided to the constructor of "java.awt.Color" in Java; it
      can also be trivially provided to UIColor's "+colorWithRed:green:blue:alpha"
      method in iOS; and, with just a little work, it can be easily formatted into
      a CSS "rgba()" string in JavaScript, as well. Here are some examples:

      Example (Java):

           import com.google.type.Color;

           // ...
           public static java.awt.Color fromProto(Color protocolor) {
             float alpha = protocolor.hasAlpha()
                 ? protocolor.getAlpha().getValue()
                 : 1.0;

             return new java.awt.Color(
                 protocolor.getRed(),
                 protocolor.getGreen(),
                 protocolor.getBlue(),
                 alpha);
           }

           public static Color toProto(java.awt.Color color) {
             float red = (float) color.getRed();
             float green = (float) color.getGreen();
             float blue = (float) color.getBlue();
             float denominator = 255.0;
             Color.Builder resultBuilder =
                 Color
                     .newBuilder()
                     .setRed(red / denominator)
                     .setGreen(green / denominator)
                     .setBlue(blue / denominator);
             int alpha = color.getAlpha();
             if (alpha != 255) {
               result.setAlpha(
                   FloatValue
                       .newBuilder()
                       .setValue(((float) alpha) / denominator)
                       .build());
             }
             return resultBuilder.build();
           }
           // ...

      Example (iOS / Obj-C):

           // ...
           static UIColor* fromProto(Color* protocolor) {
              float red = [protocolor red];
              float green = [protocolor green];
              float blue = [protocolor blue];
              FloatValue* alpha_wrapper = [protocolor alpha];
              float alpha = 1.0;
              if (alpha_wrapper != nil) {
                alpha = [alpha_wrapper value];
              }
              return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
           }

           static Color* toProto(UIColor* color) {
               CGFloat red, green, blue, alpha;
               if (![color getRed:&red green:&green blue:&blue alpha:&alpha]) {
                 return nil;
               }
               Color* result = [Color alloc] init];
               [result setRed:red];
               [result setGreen:green];
               [result setBlue:blue];
               if (alpha <= 0.9999) {
                 [result setAlpha:floatWrapperWithValue(alpha)];
               }
               [result autorelease];
               return result;
          }
          // ...

       Example (JavaScript):

          // ...

          var protoToCssColor = function(rgb_color) {
             var redFrac = rgb_color.red || 0.0;
             var greenFrac = rgb_color.green || 0.0;
             var blueFrac = rgb_color.blue || 0.0;
             var red = Math.floor(redFrac * 255);
             var green = Math.floor(greenFrac * 255);
             var blue = Math.floor(blueFrac * 255);

             if (!('alpha' in rgb_color)) {
                return rgbToCssColor_(red, green, blue);
             }

             var alphaFrac = rgb_color.alpha.value || 0.0;
             var rgbParams = [red, green, blue].join(',');
             return ['rgba(', rgbParams, ',', alphaFrac, ')'].join('');
          };

          var rgbToCssColor_ = function(red, green, blue) {
            var rgbNumber = new Number((red << 16) | (green << 8) | blue);
            var hexString = rgbNumber.toString(16);
            var missingZeros = 6 - hexString.length;
            var resultBuilder = ['#'];
            for (var i = 0; i < missingZeros; i++) {
               resultBuilder.push('0');
            }
            resultBuilder.push(hexString);
            return resultBuilder.join('');
          };

          // ...
    properties:
      alpha:
        description: |-
          The fraction of this color that should be applied to the pixel. That is,
          the final pixel color is defined by the equation:

            pixel color = alpha * (this color) + (1.0 - alpha) * (background color)

          This means that a value of 1.0 corresponds to a solid color, whereas
          a value of 0.0 corresponds to a completely transparent color. This
          uses a wrapper message rather than a simple float scalar so that it is
          possible to distinguish between a default value and the value being unset.
          If omitted, this color object is to be rendered as a solid color
          (as if the alpha value had been explicitly given with a value of 1.0).
        format: float
        type: number
      blue:
        description: 'The amount of blue in the color as a value in the interval [0, 1].'
        format: float
        type: number
      green:
        description: 'The amount of green in the color as a value in the interval [0, 1].'
        format: float
        type: number
      red:
        description: 'The amount of red in the color as a value in the interval [0, 1].'
        format: float
        type: number
    type: object
  ColorInfo:
    description: |-
      Color information consists of RGB channels, score, and the fraction of
      the image that the color occupies in the image.
    properties:
      color:
        $ref: '#/definitions/Color'
        description: RGB components of the color.
      pixelFraction:
        description: |-
          The fraction of pixels the color occupies in the image.
          Value in range [0, 1].
        format: float
        type: number
      score:
        description: 'Image-specific score for this color. Value in range [0, 1].'
        format: float
        type: number
    type: object
  CropHint:
    description: Single crop hint that is used to generate a new crop when serving an image.
    properties:
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding polygon for the crop region. The coordinates of the bounding
          box are in the original image's scale, as returned in `ImageParams`.
      confidence:
        description: 'Confidence of this being a salient region.  Range [0, 1].'
        format: float
        type: number
      importanceFraction:
        description: |-
          Fraction of importance of this salient region with respect to the original
          image.
        format: float
        type: number
    type: object
  CropHintsAnnotation:
    description: Set of crop hints that are used to generate new crops when serving images.
    properties:
      cropHints:
        description: Crop hint results.
        items:
          $ref: '#/definitions/CropHint'
        type: array
    type: object
  CropHintsParams:
    description: Parameters for crop hints annotation request.
    properties:
      aspectRatios:
        description: |-
          Aspect ratios in floats, representing the ratio of the width to the height
          of the image. For example, if the desired aspect ratio is 4/3, the
          corresponding float value should be 1.33333.  If not specified, the
          best possible crop is returned. The number of provided aspect ratios is
          limited to a maximum of 16; any aspect ratios provided after the 16th are
          ignored.
        items:
          format: float
          type: number
        type: array
    type: object
  DetectedBreak:
    description: Detected start or end of a structural component.
    properties:
      isPrefix:
        description: True if break prepends the element.
        type: boolean
      type:
        description: Detected break type.
        enum:
          - UNKNOWN
          - SPACE
          - SURE_SPACE
          - EOL_SURE_SPACE
          - HYPHEN
          - LINE_BREAK
        type: string
    type: object
  DetectedLanguage:
    description: Detected language for a structural component.
    properties:
      confidence:
        description: 'Confidence of detected language. Range [0, 1].'
        format: float
        type: number
      languageCode:
        description: |-
          The BCP-47 language code, such as "en-US" or "sr-Latn". For more
          information, see
          http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        type: string
    type: object
  DominantColorsAnnotation:
    description: Set of dominant colors and their corresponding scores.
    properties:
      colors:
        description: RGB color values with their score and pixel fraction.
        items:
          $ref: '#/definitions/ColorInfo'
        type: array
    type: object
  Empty:
    description: |-
      A generic empty message that you can re-use to avoid defining duplicated
      empty messages in your APIs. A typical example is to use it as the request
      or the response type of an API method. For instance:

          service Foo {
            rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);
          }

      The JSON representation for `Empty` is empty JSON object `{}`.
    properties: {}
    type: object
  EntityAnnotation:
    description: Set of detected entity features.
    properties:
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          Image region to which this entity belongs. Not produced
          for `LABEL_DETECTION` features.
      confidence:
        description: |-
          **Deprecated. Use `score` instead.**
          The accuracy of the entity detection in an image.
          For example, for an image in which the "Eiffel Tower" entity is detected,
          this field represents the confidence that there is a tower in the query
          image. Range [0, 1].
        format: float
        type: number
      description:
        description: 'Entity textual description, expressed in its `locale` language.'
        type: string
      locale:
        description: |-
          The language code for the locale in which the entity textual
          `description` is expressed.
        type: string
      locations:
        description: |-
          The location information for the detected entity. Multiple
          `LocationInfo` elements can be present because one location may
          indicate the location of the scene in the image, and another location
          may indicate the location of the place where the image was taken.
          Location information is usually present for landmarks.
        items:
          $ref: '#/definitions/LocationInfo'
        type: array
      mid:
        description: |-
          Opaque entity ID. Some IDs may be available in
          [Google Knowledge Graph Search
          API](https://developers.google.com/knowledge-graph/).
        type: string
      properties:
        description: |-
          Some entities may have optional user-supplied `Property` (name/value)
          fields, such a score or string that qualifies the entity.
        items:
          $ref: '#/definitions/Property'
        type: array
      score:
        description: 'Overall score of the result. Range [0, 1].'
        format: float
        type: number
      topicality:
        description: |-
          The relevancy of the ICA (Image Content Annotation) label to the
          image. For example, the relevancy of "tower" is likely higher to an image
          containing the detected "Eiffel Tower" than to an image containing a
          detected distant towering building, even though the confidence that
          there is a tower in each image may be the same. Range [0, 1].
        format: float
        type: number
    type: object
  FaceAnnotation:
    description: A face annotation object contains the results of face detection.
    properties:
      angerLikelihood:
        description: Anger likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      blurredLikelihood:
        description: Blurred likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding polygon around the face. The coordinates of the bounding box
          are in the original image's scale, as returned in `ImageParams`.
          The bounding box is computed to "frame" the face in accordance with human
          expectations. It is based on the landmarker results.
          Note that one or more x and/or y coordinates may not be generated in the
          `BoundingPoly` (the polygon will be unbounded) if only a partial face
          appears in the image to be annotated.
      detectionConfidence:
        description: 'Detection confidence. Range [0, 1].'
        format: float
        type: number
      fdBoundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The `fd_bounding_poly` bounding polygon is tighter than the
          `boundingPoly`, and encloses only the skin part of the face. Typically, it
          is used to eliminate the face from any image analysis that detects the
          "amount of skin" visible in an image. It is not based on the
          landmarker results, only on the initial face detection, hence
          the <code>fd</code> (face detection) prefix.
      headwearLikelihood:
        description: Headwear likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      joyLikelihood:
        description: Joy likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      landmarkingConfidence:
        description: 'Face landmarking confidence. Range [0, 1].'
        format: float
        type: number
      landmarks:
        description: Detected face landmarks.
        items:
          $ref: '#/definitions/Landmark'
        type: array
      panAngle:
        description: |-
          Yaw angle, which indicates the leftward/rightward angle that the face is
          pointing relative to the vertical plane perpendicular to the image. Range
          [-180,180].
        format: float
        type: number
      rollAngle:
        description: |-
          Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
          of the face relative to the image vertical about the axis perpendicular to
          the face. Range [-180,180].
        format: float
        type: number
      sorrowLikelihood:
        description: Sorrow likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      surpriseLikelihood:
        description: Surprise likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      tiltAngle:
        description: |-
          Pitch angle, which indicates the upwards/downwards angle that the face is
          pointing relative to the image's horizontal plane. Range [-180,180].
        format: float
        type: number
      underExposedLikelihood:
        description: Under-exposed likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  Feature:
    description: |-
      The type of Google Cloud Vision API detection to perform, and the maximum
      number of results to return for that type. Multiple `Feature` objects can
      be specified in the `features` list.
    properties:
      maxResults:
        description: |-
          Maximum number of results of this type. Does not apply to
          `TEXT_DETECTION`, `DOCUMENT_TEXT_DETECTION`, or `CROP_HINTS`.
        format: int32
        type: integer
      model:
        description: |-
          Model to use for the feature.
          Supported values: "builtin/stable" (the default if unset) and
          "builtin/latest".
        type: string
      type:
        description: The feature type.
        enum:
          - TYPE_UNSPECIFIED
          - FACE_DETECTION
          - LANDMARK_DETECTION
          - LOGO_DETECTION
          - LABEL_DETECTION
          - TEXT_DETECTION
          - DOCUMENT_TEXT_DETECTION
          - SAFE_SEARCH_DETECTION
          - IMAGE_PROPERTIES
          - CROP_HINTS
          - WEB_DETECTION
        type: string
    type: object
  GcsDestination:
    description: The Google Cloud Storage location where the output will be written to.
    properties:
      uri:
        description: |-
          Google Cloud Storage URI where the results will be stored. Results will
          be in JSON format and preceded by its corresponding input URI. This field
          can either represent a single file, or a prefix for multiple outputs.
          Prefixes must end in a `/`.

          Examples:

          *    File: gs://bucket-name/filename.json
          *    Prefix: gs://bucket-name/prefix/here/
          *    File: gs://bucket-name/prefix/here

          If multiple outputs, each response is still AnnotateFileResponse, each of
          which contains some subset of the full list of AnnotateImageResponse.
          Multiple outputs can happen if, for example, the output JSON is too large
          and overflows into multiple sharded files.
        type: string
    type: object
  GcsSource:
    description: The Google Cloud Storage location where the input will be read from.
    properties:
      uri:
        description: |-
          Google Cloud Storage URI for the input file. This must only be a
          Google Cloud Storage object. Wildcards are not currently supported.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1AnnotateFileResponse:
    description: |-
      Response to a single file annotation request. A file may contain one or more
      images, which individually have their own responses.
    properties:
      inputConfig:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1InputConfig'
        description: Information about the file for which this response is generated.
      responses:
        description: Individual responses to images found within the file.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1AnnotateImageResponse'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1AnnotateImageResponse:
    description: Response to an image annotation request.
    properties:
      context:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1ImageAnnotationContext'
        description: |-
          If present, contextual information is needed to understand where this image
          comes from.
      cropHintsAnnotation:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1CropHintsAnnotation'
        description: 'If present, crop hints have completed successfully.'
      error:
        $ref: '#/definitions/Status'
        description: |-
          If set, represents the error message for the operation.
          Note that filled-in image annotations are guaranteed to be
          correct, even when `error` is set.
      faceAnnotations:
        description: 'If present, face detection has completed successfully.'
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1FaceAnnotation'
        type: array
      fullTextAnnotation:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotation'
        description: |-
          If present, text (OCR) detection or document (OCR) text detection has
          completed successfully.
          This annotation provides the structural hierarchy for the OCR detected
          text.
      imagePropertiesAnnotation:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1ImageProperties'
        description: 'If present, image properties were extracted successfully.'
      labelAnnotations:
        description: 'If present, label detection has completed successfully.'
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1EntityAnnotation'
        type: array
      landmarkAnnotations:
        description: 'If present, landmark detection has completed successfully.'
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1EntityAnnotation'
        type: array
      logoAnnotations:
        description: 'If present, logo detection has completed successfully.'
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1EntityAnnotation'
        type: array
      safeSearchAnnotation:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1SafeSearchAnnotation'
        description: 'If present, safe-search annotation has completed successfully.'
      textAnnotations:
        description: 'If present, text (OCR) detection has completed successfully.'
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1EntityAnnotation'
        type: array
      webDetection:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetection'
        description: 'If present, web detection has completed successfully.'
    type: object
  GoogleCloudVisionV1p2beta1AsyncAnnotateFileResponse:
    description: The response for a single offline file annotation request.
    properties:
      outputConfig:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1OutputConfig'
        description: The output location and metadata from AsyncAnnotateFileRequest.
    type: object
  GoogleCloudVisionV1p2beta1AsyncBatchAnnotateFilesResponse:
    description: Response to an async batch file annotation request.
    properties:
      responses:
        description: |-
          The list of file annotation responses, one for each request in
          AsyncBatchAnnotateFilesRequest.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1AsyncAnnotateFileResponse'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1Block:
    description: Logical element on the page.
    properties:
      blockType:
        description: 'Detected block type (text, image etc) for this block.'
        enum:
          - UNKNOWN
          - TEXT
          - TABLE
          - PICTURE
          - RULER
          - BARCODE
        type: string
      boundingBox:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The bounding box for the block.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:

          * when the text is horizontal it might look like:

                  0----1
                  |    |
                  3----2

          * when it's rotated 180 degrees around the top-left corner it becomes:

                  2----3
                  |    |
                  1----0

            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results on the block. Range [0, 1].'
        format: float
        type: number
      paragraphs:
        description: List of paragraphs in this block (if this blocks is of type text).
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Paragraph'
        type: array
      property:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationTextProperty'
        description: Additional information detected for the block.
    type: object
  GoogleCloudVisionV1p2beta1BoundingPoly:
    description: A bounding polygon for the detected image annotation.
    properties:
      normalizedVertices:
        description: The bounding polygon normalized vertices.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1NormalizedVertex'
        type: array
      vertices:
        description: The bounding polygon vertices.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Vertex'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1ColorInfo:
    description: |-
      Color information consists of RGB channels, score, and the fraction of
      the image that the color occupies in the image.
    properties:
      color:
        $ref: '#/definitions/Color'
        description: RGB components of the color.
      pixelFraction:
        description: |-
          The fraction of pixels the color occupies in the image.
          Value in range [0, 1].
        format: float
        type: number
      score:
        description: 'Image-specific score for this color. Value in range [0, 1].'
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p2beta1CropHint:
    description: Single crop hint that is used to generate a new crop when serving an image.
    properties:
      boundingPoly:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The bounding polygon for the crop region. The coordinates of the bounding
          box are in the original image's scale, as returned in `ImageParams`.
      confidence:
        description: 'Confidence of this being a salient region.  Range [0, 1].'
        format: float
        type: number
      importanceFraction:
        description: |-
          Fraction of importance of this salient region with respect to the original
          image.
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p2beta1CropHintsAnnotation:
    description: Set of crop hints that are used to generate new crops when serving images.
    properties:
      cropHints:
        description: Crop hint results.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1CropHint'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1DominantColorsAnnotation:
    description: Set of dominant colors and their corresponding scores.
    properties:
      colors:
        description: RGB color values with their score and pixel fraction.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1ColorInfo'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1EntityAnnotation:
    description: Set of detected entity features.
    properties:
      boundingPoly:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          Image region to which this entity belongs. Not produced
          for `LABEL_DETECTION` features.
      confidence:
        description: |-
          **Deprecated. Use `score` instead.**
          The accuracy of the entity detection in an image.
          For example, for an image in which the "Eiffel Tower" entity is detected,
          this field represents the confidence that there is a tower in the query
          image. Range [0, 1].
        format: float
        type: number
      description:
        description: 'Entity textual description, expressed in its `locale` language.'
        type: string
      locale:
        description: |-
          The language code for the locale in which the entity textual
          `description` is expressed.
        type: string
      locations:
        description: |-
          The location information for the detected entity. Multiple
          `LocationInfo` elements can be present because one location may
          indicate the location of the scene in the image, and another location
          may indicate the location of the place where the image was taken.
          Location information is usually present for landmarks.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1LocationInfo'
        type: array
      mid:
        description: |-
          Opaque entity ID. Some IDs may be available in
          [Google Knowledge Graph Search
          API](https://developers.google.com/knowledge-graph/).
        type: string
      properties:
        description: |-
          Some entities may have optional user-supplied `Property` (name/value)
          fields, such a score or string that qualifies the entity.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Property'
        type: array
      score:
        description: 'Overall score of the result. Range [0, 1].'
        format: float
        type: number
      topicality:
        description: |-
          The relevancy of the ICA (Image Content Annotation) label to the
          image. For example, the relevancy of "tower" is likely higher to an image
          containing the detected "Eiffel Tower" than to an image containing a
          detected distant towering building, even though the confidence that
          there is a tower in each image may be the same. Range [0, 1].
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p2beta1FaceAnnotation:
    description: A face annotation object contains the results of face detection.
    properties:
      angerLikelihood:
        description: Anger likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      blurredLikelihood:
        description: Blurred likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      boundingPoly:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The bounding polygon around the face. The coordinates of the bounding box
          are in the original image's scale, as returned in `ImageParams`.
          The bounding box is computed to "frame" the face in accordance with human
          expectations. It is based on the landmarker results.
          Note that one or more x and/or y coordinates may not be generated in the
          `BoundingPoly` (the polygon will be unbounded) if only a partial face
          appears in the image to be annotated.
      detectionConfidence:
        description: 'Detection confidence. Range [0, 1].'
        format: float
        type: number
      fdBoundingPoly:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The `fd_bounding_poly` bounding polygon is tighter than the
          `boundingPoly`, and encloses only the skin part of the face. Typically, it
          is used to eliminate the face from any image analysis that detects the
          "amount of skin" visible in an image. It is not based on the
          landmarker results, only on the initial face detection, hence
          the <code>fd</code> (face detection) prefix.
      headwearLikelihood:
        description: Headwear likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      joyLikelihood:
        description: Joy likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      landmarkingConfidence:
        description: 'Face landmarking confidence. Range [0, 1].'
        format: float
        type: number
      landmarks:
        description: Detected face landmarks.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1FaceAnnotationLandmark'
        type: array
      panAngle:
        description: |-
          Yaw angle, which indicates the leftward/rightward angle that the face is
          pointing relative to the vertical plane perpendicular to the image. Range
          [-180,180].
        format: float
        type: number
      rollAngle:
        description: |-
          Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
          of the face relative to the image vertical about the axis perpendicular to
          the face. Range [-180,180].
        format: float
        type: number
      sorrowLikelihood:
        description: Sorrow likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      surpriseLikelihood:
        description: Surprise likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      tiltAngle:
        description: |-
          Pitch angle, which indicates the upwards/downwards angle that the face is
          pointing relative to the image's horizontal plane. Range [-180,180].
        format: float
        type: number
      underExposedLikelihood:
        description: Under-exposed likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  GoogleCloudVisionV1p2beta1FaceAnnotationLandmark:
    description: 'A face-specific landmark (for example, a face feature).'
    properties:
      position:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1Position'
        description: Face landmark position.
      type:
        description: Face landmark type.
        enum:
          - UNKNOWN_LANDMARK
          - LEFT_EYE
          - RIGHT_EYE
          - LEFT_OF_LEFT_EYEBROW
          - RIGHT_OF_LEFT_EYEBROW
          - LEFT_OF_RIGHT_EYEBROW
          - RIGHT_OF_RIGHT_EYEBROW
          - MIDPOINT_BETWEEN_EYES
          - NOSE_TIP
          - UPPER_LIP
          - LOWER_LIP
          - MOUTH_LEFT
          - MOUTH_RIGHT
          - MOUTH_CENTER
          - NOSE_BOTTOM_RIGHT
          - NOSE_BOTTOM_LEFT
          - NOSE_BOTTOM_CENTER
          - LEFT_EYE_TOP_BOUNDARY
          - LEFT_EYE_RIGHT_CORNER
          - LEFT_EYE_BOTTOM_BOUNDARY
          - LEFT_EYE_LEFT_CORNER
          - RIGHT_EYE_TOP_BOUNDARY
          - RIGHT_EYE_RIGHT_CORNER
          - RIGHT_EYE_BOTTOM_BOUNDARY
          - RIGHT_EYE_LEFT_CORNER
          - LEFT_EYEBROW_UPPER_MIDPOINT
          - RIGHT_EYEBROW_UPPER_MIDPOINT
          - LEFT_EAR_TRAGION
          - RIGHT_EAR_TRAGION
          - LEFT_EYE_PUPIL
          - RIGHT_EYE_PUPIL
          - FOREHEAD_GLABELLA
          - CHIN_GNATHION
          - CHIN_LEFT_GONION
          - CHIN_RIGHT_GONION
        type: string
    type: object
  GoogleCloudVisionV1p2beta1GcsDestination:
    description: The Google Cloud Storage location where the output will be written to.
    properties:
      uri:
        description: |-
          Google Cloud Storage URI where the results will be stored. Results will
          be in JSON format and preceded by its corresponding input URI. This field
          can either represent a single file, or a prefix for multiple outputs.
          Prefixes must end in a `/`.

          Examples:

          *    File: gs://bucket-name/filename.json
          *    Prefix: gs://bucket-name/prefix/here/
          *    File: gs://bucket-name/prefix/here

          If multiple outputs, each response is still AnnotateFileResponse, each of
          which contains some subset of the full list of AnnotateImageResponse.
          Multiple outputs can happen if, for example, the output JSON is too large
          and overflows into multiple sharded files.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1GcsSource:
    description: The Google Cloud Storage location where the input will be read from.
    properties:
      uri:
        description: |-
          Google Cloud Storage URI for the input file. This must only be a
          Google Cloud Storage object. Wildcards are not currently supported.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1ImageAnnotationContext:
    description: |-
      If an image was produced from a file (e.g. a PDF), this message gives
      information about the source of that image.
    properties:
      pageNumber:
        description: |-
          If the file was a PDF or TIFF, this field gives the page number within
          the file used to produce the image.
        format: int32
        type: integer
      uri:
        description: The URI of the file used to produce the image.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1ImageProperties:
    description: 'Stores image properties, such as dominant colors.'
    properties:
      dominantColors:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1DominantColorsAnnotation'
        description: 'If present, dominant colors completed successfully.'
    type: object
  GoogleCloudVisionV1p2beta1InputConfig:
    description: The desired input location and metadata.
    properties:
      gcsSource:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1GcsSource'
        description: The Google Cloud Storage location to read the input from.
      mimeType:
        description: |-
          The type of the file. Currently only "application/pdf" and "image/tiff"
          are supported. Wildcards are not supported.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1LocationInfo:
    description: Detected entity location information.
    properties:
      latLng:
        $ref: '#/definitions/LatLng'
        description: lat/long location coordinates.
    type: object
  GoogleCloudVisionV1p2beta1NormalizedVertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the normalized vertex coordinates are relative to the original image
      and range from 0 to 1.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p2beta1OperationMetadata:
    description: Contains metadata for the BatchAnnotateImages operation.
    properties:
      createTime:
        description: The time when the batch request was received.
        format: google-datetime
        type: string
      state:
        description: Current state of the batch operation.
        enum:
          - STATE_UNSPECIFIED
          - CREATED
          - RUNNING
          - DONE
          - CANCELLED
        type: string
      updateTime:
        description: The time when the operation result was last updated.
        format: google-datetime
        type: string
    type: object
  GoogleCloudVisionV1p2beta1OutputConfig:
    description: The desired output location and metadata.
    properties:
      batchSize:
        description: |-
          The max number of response protos to put into each output JSON file on
          Google Cloud Storage.
          The valid range is [1, 100]. If not specified, the default value is 20.

          For example, for one pdf file with 100 pages, 100 response protos will
          be generated. If `batch_size` = 20, then 5 json files each
          containing 20 response protos will be written under the prefix
          `gcs_destination`.`uri`.

          Currently, batch_size only applies to GcsDestination, with potential future
          support for other output configurations.
        format: int32
        type: integer
      gcsDestination:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1GcsDestination'
        description: The Google Cloud Storage location to write the output(s) to.
    type: object
  GoogleCloudVisionV1p2beta1Page:
    description: Detected page from OCR.
    properties:
      blocks:
        description: 'List of blocks of text, images etc on this page.'
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Block'
        type: array
      confidence:
        description: 'Confidence of the OCR results on the page. Range [0, 1].'
        format: float
        type: number
      height:
        description: |-
          Page height. For PDFs the unit is points. For images (including
          TIFFs) the unit is pixels.
        format: int32
        type: integer
      property:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationTextProperty'
        description: Additional information detected on the page.
      width:
        description: |-
          Page width. For PDFs the unit is points. For images (including
          TIFFs) the unit is pixels.
        format: int32
        type: integer
    type: object
  GoogleCloudVisionV1p2beta1Paragraph:
    description: Structural unit of text representing a number of words in certain order.
    properties:
      boundingBox:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The bounding box for the paragraph.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results for the paragraph. Range [0, 1].'
        format: float
        type: number
      property:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationTextProperty'
        description: Additional information detected for the paragraph.
      words:
        description: List of words in this paragraph.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Word'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1Position:
    description: |-
      A 3D position in the image, used primarily for Face detection landmarks.
      A valid Position must have both x and y coordinates.
      The position coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
      z:
        description: Z coordinate (or depth).
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p2beta1Property:
    description: A `Property` consists of a user-supplied name/value pair.
    properties:
      name:
        description: Name of the property.
        type: string
      uint64Value:
        description: Value of numeric properties.
        format: uint64
        type: string
      value:
        description: Value of the property.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1SafeSearchAnnotation:
    description: |-
      Set of features pertaining to the image, computed by computer vision
      methods over safe-search verticals (for example, adult, spoof, medical,
      violence).
    properties:
      adult:
        description: |-
          Represents the adult content likelihood for the image. Adult content may
          contain elements such as nudity, pornographic images or cartoons, or
          sexual activities.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      medical:
        description: Likelihood that this is a medical image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      racy:
        description: |-
          Likelihood that the request image contains racy content. Racy content may
          include (but is not limited to) skimpy or sheer clothing, strategically
          covered nudity, lewd or provocative poses, or close-ups of sensitive
          body areas.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      spoof:
        description: |-
          Spoof likelihood. The likelihood that an modification
          was made to the image's canonical version to make it appear
          funny or offensive.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      violence:
        description: Likelihood that this image contains violent content.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  GoogleCloudVisionV1p2beta1Symbol:
    description: A single symbol representation.
    properties:
      boundingBox:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The bounding box for the symbol.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results for the symbol. Range [0, 1].'
        format: float
        type: number
      property:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationTextProperty'
        description: Additional information detected for the symbol.
      text:
        description: The actual UTF-8 representation of the symbol.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1TextAnnotation:
    description: |-
      TextAnnotation contains a structured representation of OCR extracted text.
      The hierarchy of an OCR extracted text structure is like this:
          TextAnnotation -> Page -> Block -> Paragraph -> Word -> Symbol
      Each structural component, starting from Page, may further have their own
      properties. Properties describe detected languages, breaks etc.. Please refer
      to the TextAnnotation.TextProperty message definition below for more
      detail.
    properties:
      pages:
        description: List of pages detected by OCR.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Page'
        type: array
      text:
        description: UTF-8 text detected on the pages.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1TextAnnotationDetectedBreak:
    description: Detected start or end of a structural component.
    properties:
      isPrefix:
        description: True if break prepends the element.
        type: boolean
      type:
        description: Detected break type.
        enum:
          - UNKNOWN
          - SPACE
          - SURE_SPACE
          - EOL_SURE_SPACE
          - HYPHEN
          - LINE_BREAK
        type: string
    type: object
  GoogleCloudVisionV1p2beta1TextAnnotationDetectedLanguage:
    description: Detected language for a structural component.
    properties:
      confidence:
        description: 'Confidence of detected language. Range [0, 1].'
        format: float
        type: number
      languageCode:
        description: |-
          The BCP-47 language code, such as "en-US" or "sr-Latn". For more
          information, see
          http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1TextAnnotationTextProperty:
    description: Additional information detected on the structural component.
    properties:
      detectedBreak:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationDetectedBreak'
        description: Detected start or end of a text segment.
      detectedLanguages:
        description: A list of detected languages together with confidence.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationDetectedLanguage'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1Vertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the vertex coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: int32
        type: integer
      'y':
        description: Y coordinate.
        format: int32
        type: integer
    type: object
  GoogleCloudVisionV1p2beta1WebDetection:
    description: Relevant information for the image from the Internet.
    properties:
      bestGuessLabels:
        description: Best guess text labels for the request image.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebLabel'
        type: array
      fullMatchingImages:
        description: |-
          Fully matching images from the Internet.
          Can include resized copies of the query image.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebImage'
        type: array
      pagesWithMatchingImages:
        description: Web pages containing the matching images from the Internet.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebPage'
        type: array
      partialMatchingImages:
        description: |-
          Partial matching images from the Internet.
          Those images are similar enough to share some key-point features. For
          example an original image will likely have partial matching for its crops.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebImage'
        type: array
      visuallySimilarImages:
        description: The visually similar image results.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebImage'
        type: array
      webEntities:
        description: Deduced entities from similar images on the Internet.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebEntity'
        type: array
    type: object
  GoogleCloudVisionV1p2beta1WebDetectionWebEntity:
    description: Entity deduced from similar images on the Internet.
    properties:
      description:
        description: 'Canonical description of the entity, in English.'
        type: string
      entityId:
        description: Opaque entity ID.
        type: string
      score:
        description: |-
          Overall relevancy score for the entity.
          Not normalized and not comparable across different image queries.
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p2beta1WebDetectionWebImage:
    description: Metadata for online images.
    properties:
      score:
        description: (Deprecated) Overall relevancy score for the image.
        format: float
        type: number
      url:
        description: The result image URL.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1WebDetectionWebLabel:
    description: Label to provide extra metadata for the web detection.
    properties:
      label:
        description: Label for extra metadata.
        type: string
      languageCode:
        description: |-
          The BCP-47 language code for `label`, such as "en-US" or "sr-Latn".
          For more information, see
          http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1WebDetectionWebPage:
    description: Metadata for web pages.
    properties:
      fullMatchingImages:
        description: |-
          Fully matching images on the page.
          Can include resized copies of the query image.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebImage'
        type: array
      pageTitle:
        description: 'Title for the web page, may contain HTML markups.'
        type: string
      partialMatchingImages:
        description: |-
          Partial matching images on the page.
          Those images are similar enough to share some key-point features. For
          example an original image will likely have partial matching for its
          crops.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1WebDetectionWebImage'
        type: array
      score:
        description: (Deprecated) Overall relevancy score for the web page.
        format: float
        type: number
      url:
        description: The result web page URL.
        type: string
    type: object
  GoogleCloudVisionV1p2beta1Word:
    description: A word representation.
    properties:
      boundingBox:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1BoundingPoly'
        description: |-
          The bounding box for the word.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results for the word. Range [0, 1].'
        format: float
        type: number
      property:
        $ref: '#/definitions/GoogleCloudVisionV1p2beta1TextAnnotationTextProperty'
        description: Additional information detected for the word.
      symbols:
        description: |-
          List of symbols in the word.
          The order of the symbols follows the natural reading order.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p2beta1Symbol'
        type: array
    type: object
  GoogleCloudVisionV1p3beta1BatchOperationMetadata:
    description: |-
      Metadata for the batch operations such as the current state.

      This is included in the `metadata` field of the `Operation` returned by the
      `GetOperation` call of the `google::longrunning::Operations` service.
    properties:
      endTime:
        description: |-
          The time when the batch request is finished and
          google.longrunning.Operation.done is set to true.
        format: google-datetime
        type: string
      state:
        description: The current state of the batch operation.
        enum:
          - STATE_UNSPECIFIED
          - PROCESSING
          - SUCCESSFUL
          - FAILED
          - CANCELLED
        type: string
      submitTime:
        description: The time when the batch request was submitted to the server.
        format: google-datetime
        type: string
    type: object
  GoogleCloudVisionV1p3beta1BoundingPoly:
    description: A bounding polygon for the detected image annotation.
    properties:
      normalizedVertices:
        description: The bounding polygon normalized vertices.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p3beta1NormalizedVertex'
        type: array
      vertices:
        description: The bounding polygon vertices.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p3beta1Vertex'
        type: array
    type: object
  GoogleCloudVisionV1p3beta1ImportProductSetsResponse:
    description: |-
      Response message for the `ImportProductSets` method.

      This message is returned by the
      google.longrunning.Operations.GetOperation method in the returned
      google.longrunning.Operation.response field.
    properties:
      referenceImages:
        description: The list of reference_images that are imported successfully.
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p3beta1ReferenceImage'
        type: array
      statuses:
        description: |-
          The rpc status for each ImportProductSet request, including both successes
          and errors.

          The number of statuses here matches the number of lines in the csv file,
          and statuses[i] stores the success or failure status of processing the i-th
          line of the csv, starting from line 0.
        items:
          $ref: '#/definitions/Status'
        type: array
    type: object
  GoogleCloudVisionV1p3beta1NormalizedVertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the normalized vertex coordinates are relative to the original image
      and range from 0 to 1.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
    type: object
  GoogleCloudVisionV1p3beta1ReferenceImage:
    description: |-
      A `ReferenceImage` represents a product image and its associated metadata,
      such as bounding boxes.
    properties:
      boundingPolys:
        description: |-
          Bounding polygons around the areas of interest in the reference image.
          Optional. If this field is empty, the system will try to detect regions of
          interest. At most 10 bounding polygons will be used.

          The provided shape is converted into a non-rotated rectangle. Once
          converted, the small edge of the rectangle must be greater than or equal
          to 300 pixels. The aspect ratio must be 1:4 or less (i.e. 1:3 is ok; 1:5
          is not).
        items:
          $ref: '#/definitions/GoogleCloudVisionV1p3beta1BoundingPoly'
        type: array
      name:
        description: |-
          The resource name of the reference image.

          Format is:

          `projects/PROJECT_ID/locations/LOC_ID/products/PRODUCT_ID/referenceImages/IMAGE_ID`.

          This field is ignored when creating a reference image.
        type: string
      uri:
        description: |-
          The Google Cloud Storage URI of the reference image.

          The URI must start with `gs://`.

          Required.
        type: string
    type: object
  GoogleCloudVisionV1p3beta1Vertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the vertex coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: int32
        type: integer
      'y':
        description: Y coordinate.
        format: int32
        type: integer
    type: object
  Image:
    description: Client image to perform Google Cloud Vision API tasks over.
    properties:
      content:
        description: |-
          Image content, represented as a stream of bytes.
          Note: As with all `bytes` fields, protobuffers use a pure binary
          representation, whereas JSON representations use base64.
        format: byte
        type: string
      source:
        $ref: '#/definitions/ImageSource'
        description: |-
          Google Cloud Storage image location, or publicly-accessible image
          URL. If both `content` and `source` are provided for an image, `content`
          takes precedence and is used to perform the image annotation request.
    type: object
  ImageAnnotationContext:
    description: |-
      If an image was produced from a file (e.g. a PDF), this message gives
      information about the source of that image.
    properties:
      pageNumber:
        description: |-
          If the file was a PDF or TIFF, this field gives the page number within
          the file used to produce the image.
        format: int32
        type: integer
      uri:
        description: The URI of the file used to produce the image.
        type: string
    type: object
  ImageContext:
    description: Image context and/or feature-specific parameters.
    properties:
      cropHintsParams:
        $ref: '#/definitions/CropHintsParams'
        description: Parameters for crop hints annotation request.
      languageHints:
        description: |-
          List of languages to use for TEXT_DETECTION. In most cases, an empty value
          yields the best results since it enables automatic language detection. For
          languages based on the Latin alphabet, setting `language_hints` is not
          needed. In rare cases, when the language of the text in the image is known,
          setting a hint will help get better results (although it will be a
          significant hindrance if the hint is wrong). Text detection returns an
          error if one or more of the specified languages is not one of the
          [supported languages](/vision/docs/languages).
        items:
          type: string
        type: array
      latLongRect:
        $ref: '#/definitions/LatLongRect'
        description: Not used.
      webDetectionParams:
        $ref: '#/definitions/WebDetectionParams'
        description: Parameters for web detection.
    type: object
  ImageProperties:
    description: 'Stores image properties, such as dominant colors.'
    properties:
      dominantColors:
        $ref: '#/definitions/DominantColorsAnnotation'
        description: 'If present, dominant colors completed successfully.'
    type: object
  ImageSource:
    description: External image source (Google Cloud Storage or web URL image location).
    properties:
      gcsImageUri:
        description: |-
          **Use `image_uri` instead.**

          The Google Cloud Storage  URI of the form
          `gs://bucket_name/object_name`. Object versioning is not supported. See
          [Google Cloud Storage Request
          URIs](https://cloud.google.com/storage/docs/reference-uris) for more info.
        type: string
      imageUri:
        description: |-
          The URI of the source image. Can be either:

          1. A Google Cloud Storage URI of the form
             `gs://bucket_name/object_name`. Object versioning is not supported. See
             [Google Cloud Storage Request
             URIs](https://cloud.google.com/storage/docs/reference-uris) for more
             info.

          2. A publicly-accessible image HTTP/HTTPS URL. When fetching images from
             HTTP/HTTPS URLs, Google cannot guarantee that the request will be
             completed. Your request may fail if the specified host denies the
             request (e.g. due to request throttling or DOS prevention), or if Google
             throttles requests to the site for abuse prevention. You should not
             depend on externally-hosted images for production applications.

          When both `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
          precedence.
        type: string
    type: object
  InputConfig:
    description: The desired input location and metadata.
    properties:
      gcsSource:
        $ref: '#/definitions/GcsSource'
        description: The Google Cloud Storage location to read the input from.
      mimeType:
        description: |-
          The type of the file. Currently only "application/pdf" and "image/tiff"
          are supported. Wildcards are not supported.
        type: string
    type: object
  Landmark:
    description: 'A face-specific landmark (for example, a face feature).'
    properties:
      position:
        $ref: '#/definitions/Position'
        description: Face landmark position.
      type:
        description: Face landmark type.
        enum:
          - UNKNOWN_LANDMARK
          - LEFT_EYE
          - RIGHT_EYE
          - LEFT_OF_LEFT_EYEBROW
          - RIGHT_OF_LEFT_EYEBROW
          - LEFT_OF_RIGHT_EYEBROW
          - RIGHT_OF_RIGHT_EYEBROW
          - MIDPOINT_BETWEEN_EYES
          - NOSE_TIP
          - UPPER_LIP
          - LOWER_LIP
          - MOUTH_LEFT
          - MOUTH_RIGHT
          - MOUTH_CENTER
          - NOSE_BOTTOM_RIGHT
          - NOSE_BOTTOM_LEFT
          - NOSE_BOTTOM_CENTER
          - LEFT_EYE_TOP_BOUNDARY
          - LEFT_EYE_RIGHT_CORNER
          - LEFT_EYE_BOTTOM_BOUNDARY
          - LEFT_EYE_LEFT_CORNER
          - RIGHT_EYE_TOP_BOUNDARY
          - RIGHT_EYE_RIGHT_CORNER
          - RIGHT_EYE_BOTTOM_BOUNDARY
          - RIGHT_EYE_LEFT_CORNER
          - LEFT_EYEBROW_UPPER_MIDPOINT
          - RIGHT_EYEBROW_UPPER_MIDPOINT
          - LEFT_EAR_TRAGION
          - RIGHT_EAR_TRAGION
          - LEFT_EYE_PUPIL
          - RIGHT_EYE_PUPIL
          - FOREHEAD_GLABELLA
          - CHIN_GNATHION
          - CHIN_LEFT_GONION
          - CHIN_RIGHT_GONION
        type: string
    type: object
  LatLng:
    description: |-
      An object representing a latitude/longitude pair. This is expressed as a pair
      of doubles representing degrees latitude and degrees longitude. Unless
      specified otherwise, this must conform to the
      <a href="http://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf">WGS84
      standard</a>. Values must be within normalized ranges.
    properties:
      latitude:
        description: 'The latitude in degrees. It must be in the range [-90.0, +90.0].'
        format: double
        type: number
      longitude:
        description: 'The longitude in degrees. It must be in the range [-180.0, +180.0].'
        format: double
        type: number
    type: object
  LatLongRect:
    description: Rectangle determined by min and max `LatLng` pairs.
    properties:
      maxLatLng:
        $ref: '#/definitions/LatLng'
        description: Max lat/long pair.
      minLatLng:
        $ref: '#/definitions/LatLng'
        description: Min lat/long pair.
    type: object
  ListOperationsResponse:
    description: The response message for Operations.ListOperations.
    properties:
      nextPageToken:
        description: The standard List next-page token.
        type: string
      operations:
        description: A list of operations that matches the specified filter in the request.
        items:
          $ref: '#/definitions/Operation'
        type: array
    type: object
  LocationInfo:
    description: Detected entity location information.
    properties:
      latLng:
        $ref: '#/definitions/LatLng'
        description: lat/long location coordinates.
    type: object
  NormalizedVertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the normalized vertex coordinates are relative to the original image
      and range from 0 to 1.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
    type: object
  Operation:
    description: |-
      This resource represents a long-running operation that is the result of a
      network API call.
    properties:
      done:
        description: |-
          If the value is `false`, it means the operation is still in progress.
          If `true`, the operation is completed, and either `error` or `response` is
          available.
        type: boolean
      error:
        $ref: '#/definitions/Status'
        description: The error result of the operation in case of failure or cancellation.
      metadata:
        additionalProperties:
          description: Properties of the object. Contains field @type with type URL.
        description: |-
          Service-specific metadata associated with the operation.  It typically
          contains progress information and common metadata such as create time.
          Some services might not provide such metadata.  Any method that returns a
          long-running operation should document the metadata type, if any.
        type: object
      name:
        description: |-
          The server-assigned name, which is only unique within the same service that
          originally returns it. If you use the default HTTP mapping, the
          `name` should have the format of `operations/some/unique/name`.
        type: string
      response:
        additionalProperties:
          description: Properties of the object. Contains field @type with type URL.
        description: |-
          The normal response of the operation in case of success.  If the original
          method returns no data on success, such as `Delete`, the response is
          `google.protobuf.Empty`.  If the original method is standard
          `Get`/`Create`/`Update`, the response should be the resource.  For other
          methods, the response should have the type `XxxResponse`, where `Xxx`
          is the original method name.  For example, if the original method name
          is `TakeSnapshot()`, the inferred response type is
          `TakeSnapshotResponse`.
        type: object
    type: object
  OperationMetadata:
    description: Contains metadata for the BatchAnnotateImages operation.
    properties:
      createTime:
        description: The time when the batch request was received.
        format: google-datetime
        type: string
      state:
        description: Current state of the batch operation.
        enum:
          - STATE_UNSPECIFIED
          - CREATED
          - RUNNING
          - DONE
          - CANCELLED
        type: string
      updateTime:
        description: The time when the operation result was last updated.
        format: google-datetime
        type: string
    type: object
  OutputConfig:
    description: The desired output location and metadata.
    properties:
      batchSize:
        description: |-
          The max number of response protos to put into each output JSON file on
          Google Cloud Storage.
          The valid range is [1, 100]. If not specified, the default value is 20.

          For example, for one pdf file with 100 pages, 100 response protos will
          be generated. If `batch_size` = 20, then 5 json files each
          containing 20 response protos will be written under the prefix
          `gcs_destination`.`uri`.

          Currently, batch_size only applies to GcsDestination, with potential future
          support for other output configurations.
        format: int32
        type: integer
      gcsDestination:
        $ref: '#/definitions/GcsDestination'
        description: The Google Cloud Storage location to write the output(s) to.
    type: object
  Page:
    description: Detected page from OCR.
    properties:
      blocks:
        description: 'List of blocks of text, images etc on this page.'
        items:
          $ref: '#/definitions/Block'
        type: array
      confidence:
        description: 'Confidence of the OCR results on the page. Range [0, 1].'
        format: float
        type: number
      height:
        description: |-
          Page height. For PDFs the unit is points. For images (including
          TIFFs) the unit is pixels.
        format: int32
        type: integer
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected on the page.
      width:
        description: |-
          Page width. For PDFs the unit is points. For images (including
          TIFFs) the unit is pixels.
        format: int32
        type: integer
    type: object
  Paragraph:
    description: Structural unit of text representing a number of words in certain order.
    properties:
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the paragraph.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results for the paragraph. Range [0, 1].'
        format: float
        type: number
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the paragraph.
      words:
        description: List of words in this paragraph.
        items:
          $ref: '#/definitions/Word'
        type: array
    type: object
  Position:
    description: |-
      A 3D position in the image, used primarily for Face detection landmarks.
      A valid Position must have both x and y coordinates.
      The position coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
      z:
        description: Z coordinate (or depth).
        format: float
        type: number
    type: object
  Property:
    description: A `Property` consists of a user-supplied name/value pair.
    properties:
      name:
        description: Name of the property.
        type: string
      uint64Value:
        description: Value of numeric properties.
        format: uint64
        type: string
      value:
        description: Value of the property.
        type: string
    type: object
  SafeSearchAnnotation:
    description: |-
      Set of features pertaining to the image, computed by computer vision
      methods over safe-search verticals (for example, adult, spoof, medical,
      violence).
    properties:
      adult:
        description: |-
          Represents the adult content likelihood for the image. Adult content may
          contain elements such as nudity, pornographic images or cartoons, or
          sexual activities.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      medical:
        description: Likelihood that this is a medical image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      racy:
        description: |-
          Likelihood that the request image contains racy content. Racy content may
          include (but is not limited to) skimpy or sheer clothing, strategically
          covered nudity, lewd or provocative poses, or close-ups of sensitive
          body areas.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      spoof:
        description: |-
          Spoof likelihood. The likelihood that an modification
          was made to the image's canonical version to make it appear
          funny or offensive.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      violence:
        description: Likelihood that this image contains violent content.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  Status:
    description: |-
      The `Status` type defines a logical error model that is suitable for different
      programming environments, including REST APIs and RPC APIs. It is used by
      [gRPC](https://github.com/grpc). The error model is designed to be:

      - Simple to use and understand for most users
      - Flexible enough to meet unexpected needs

      # Overview

      The `Status` message contains three pieces of data: error code, error message,
      and error details. The error code should be an enum value of
      google.rpc.Code, but it may accept additional error codes if needed.  The
      error message should be a developer-facing English message that helps
      developers *understand* and *resolve* the error. If a localized user-facing
      error message is needed, put the localized message in the error details or
      localize it in the client. The optional error details may contain arbitrary
      information about the error. There is a predefined set of error detail types
      in the package `google.rpc` that can be used for common error conditions.

      # Language mapping

      The `Status` message is the logical representation of the error model, but it
      is not necessarily the actual wire format. When the `Status` message is
      exposed in different client libraries and different wire protocols, it can be
      mapped differently. For example, it will likely be mapped to some exceptions
      in Java, but more likely mapped to some error codes in C.

      # Other uses

      The error model and the `Status` message can be used in a variety of
      environments, either with or without APIs, to provide a
      consistent developer experience across different environments.

      Example uses of this error model include:

      - Partial errors. If a service needs to return partial errors to the client,
          it may embed the `Status` in the normal response to indicate the partial
          errors.

      - Workflow errors. A typical workflow has multiple steps. Each step may
          have a `Status` message for error reporting.

      - Batch operations. If a client uses batch request and batch response, the
          `Status` message should be used directly inside batch response, one for
          each error sub-response.

      - Asynchronous operations. If an API call embeds asynchronous operation
          results in its response, the status of those operations should be
          represented directly using the `Status` message.

      - Logging. If some API errors are stored in logs, the message `Status` could
          be used directly after any stripping needed for security/privacy reasons.
    properties:
      code:
        description: 'The status code, which should be an enum value of google.rpc.Code.'
        format: int32
        type: integer
      details:
        description: |-
          A list of messages that carry the error details.  There is a common set of
          message types for APIs to use.
        items:
          additionalProperties:
            description: Properties of the object. Contains field @type with type URL.
          type: object
        type: array
      message:
        description: |-
          A developer-facing error message, which should be in English. Any
          user-facing error message should be localized and sent in the
          google.rpc.Status.details field, or localized by the client.
        type: string
    type: object
  Symbol:
    description: A single symbol representation.
    properties:
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the symbol.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results for the symbol. Range [0, 1].'
        format: float
        type: number
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the symbol.
      text:
        description: The actual UTF-8 representation of the symbol.
        type: string
    type: object
  TextAnnotation:
    description: |-
      TextAnnotation contains a structured representation of OCR extracted text.
      The hierarchy of an OCR extracted text structure is like this:
          TextAnnotation -> Page -> Block -> Paragraph -> Word -> Symbol
      Each structural component, starting from Page, may further have their own
      properties. Properties describe detected languages, breaks etc.. Please refer
      to the TextAnnotation.TextProperty message definition below for more
      detail.
    properties:
      pages:
        description: List of pages detected by OCR.
        items:
          $ref: '#/definitions/Page'
        type: array
      text:
        description: UTF-8 text detected on the pages.
        type: string
    type: object
  TextProperty:
    description: Additional information detected on the structural component.
    properties:
      detectedBreak:
        $ref: '#/definitions/DetectedBreak'
        description: Detected start or end of a text segment.
      detectedLanguages:
        description: A list of detected languages together with confidence.
        items:
          $ref: '#/definitions/DetectedLanguage'
        type: array
    type: object
  Vertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the vertex coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: int32
        type: integer
      'y':
        description: Y coordinate.
        format: int32
        type: integer
    type: object
  WebDetection:
    description: Relevant information for the image from the Internet.
    properties:
      bestGuessLabels:
        description: Best guess text labels for the request image.
        items:
          $ref: '#/definitions/WebLabel'
        type: array
      fullMatchingImages:
        description: |-
          Fully matching images from the Internet.
          Can include resized copies of the query image.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      pagesWithMatchingImages:
        description: Web pages containing the matching images from the Internet.
        items:
          $ref: '#/definitions/WebPage'
        type: array
      partialMatchingImages:
        description: |-
          Partial matching images from the Internet.
          Those images are similar enough to share some key-point features. For
          example an original image will likely have partial matching for its crops.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      visuallySimilarImages:
        description: The visually similar image results.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      webEntities:
        description: Deduced entities from similar images on the Internet.
        items:
          $ref: '#/definitions/WebEntity'
        type: array
    type: object
  WebDetectionParams:
    description: Parameters for web detection request.
    properties:
      includeGeoResults:
        description: Whether to include results derived from the geo information in the image.
        type: boolean
    type: object
  WebEntity:
    description: Entity deduced from similar images on the Internet.
    properties:
      description:
        description: 'Canonical description of the entity, in English.'
        type: string
      entityId:
        description: Opaque entity ID.
        type: string
      score:
        description: |-
          Overall relevancy score for the entity.
          Not normalized and not comparable across different image queries.
        format: float
        type: number
    type: object
  WebImage:
    description: Metadata for online images.
    properties:
      score:
        description: (Deprecated) Overall relevancy score for the image.
        format: float
        type: number
      url:
        description: The result image URL.
        type: string
    type: object
  WebLabel:
    description: Label to provide extra metadata for the web detection.
    properties:
      label:
        description: Label for extra metadata.
        type: string
      languageCode:
        description: |-
          The BCP-47 language code for `label`, such as "en-US" or "sr-Latn".
          For more information, see
          http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        type: string
    type: object
  WebPage:
    description: Metadata for web pages.
    properties:
      fullMatchingImages:
        description: |-
          Fully matching images on the page.
          Can include resized copies of the query image.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      pageTitle:
        description: 'Title for the web page, may contain HTML markups.'
        type: string
      partialMatchingImages:
        description: |-
          Partial matching images on the page.
          Those images are similar enough to share some key-point features. For
          example an original image will likely have partial matching for its
          crops.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      score:
        description: (Deprecated) Overall relevancy score for the web page.
        format: float
        type: number
      url:
        description: The result web page URL.
        type: string
    type: object
  Word:
    description: A word representation.
    properties:
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the word.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      confidence:
        description: 'Confidence of the OCR results for the word. Range [0, 1].'
        format: float
        type: number
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the word.
      symbols:
        description: |-
          List of symbols in the word.
          The order of the symbols follows the natural reading order.
        items:
          $ref: '#/definitions/Symbol'
        type: array
    type: object
