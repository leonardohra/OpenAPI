swagger: '2.0'
schemes:
  - https
host: speech.googleapis.com
basePath: /
info:
  contact:
    name: Google
    url: 'https://google.com'
  description: Converts audio to text by applying powerful neural network models.
  title: Cloud Speech
  version: v1
  x-apiClientRegistration:
    url: 'https://console.developers.google.com'
  x-apisguru-categories:
    - machine_learning
  x-logo:
    url: 'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png'
  x-origin:
    - converter:
        url: 'https://github.com/lucybot/api-spec-converter'
        version: 2.6.2
      format: google
      url: 'https://speech.googleapis.com/$discovery/rest?version=v1'
      version: v1
  x-preferred: true
  x-providerName: googleapis.com
  x-serviceName: speech
externalDocs:
  url: 'https://cloud.google.com/speech/'
securityDefinitions:
  Oauth2:
    authorizationUrl: 'https://accounts.google.com/o/oauth2/auth'
    description: Oauth 2.0 authentication
    flow: implicit
    scopes:
      'https://www.googleapis.com/auth/cloud-platform': View and manage your data across Google Cloud Platform services
    type: oauth2
parameters:
  $.xgafv:
    description: V1 error format.
    enum:
      - '1'
      - '2'
    in: query
    name: $.xgafv
    type: string
  access_token:
    description: OAuth access token.
    in: query
    name: access_token
    type: string
  alt:
    default: json
    description: Data format for response.
    enum:
      - json
      - media
      - proto
    in: query
    name: alt
    type: string
  bearer_token:
    description: OAuth bearer token.
    in: query
    name: bearer_token
    type: string
  callback:
    description: JSONP
    in: query
    name: callback
    type: string
  fields:
    description: Selector specifying which fields to include in a partial response.
    in: query
    name: fields
    type: string
  key:
    description: 'API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.'
    in: query
    name: key
    type: string
  oauth_token:
    description: OAuth 2.0 token for the current user.
    in: query
    name: oauth_token
    type: string
  pp:
    default: true
    description: Pretty-print response.
    in: query
    name: pp
    type: boolean
  prettyPrint:
    default: true
    description: Returns response with indentations and line breaks.
    in: query
    name: prettyPrint
    type: boolean
  quotaUser:
    description: 'Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.'
    in: query
    name: quotaUser
    type: string
  uploadType:
    description: 'Legacy upload protocol for media (e.g. "media", "multipart").'
    in: query
    name: uploadType
    type: string
  upload_protocol:
    description: 'Upload protocol for media (e.g. "raw", "multipart").'
    in: query
    name: upload_protocol
    type: string
tags:
  - name: operations
  - name: speech
paths:
  '/v1/operations/{name}':
    get:
      description: |-
        Gets the latest state of a long-running operation.  Clients can use this
        method to poll the operation result at intervals as recommended by the API
        service.
      operationId: speech.operations.get
      parameters:
        - description: The name of the operation resource.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - operations
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
  '/v1/speech:longrunningrecognize':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: |-
        Performs asynchronous speech recognition: receive results via the
        google.longrunning.Operations interface. Returns either an
        `Operation.error` or an `Operation.response` which contains
        a `LongRunningRecognizeResponse` message.
      operationId: speech.speech.longrunningrecognize
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/LongRunningRecognizeRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - speech
  '/v1/speech:recognize':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: |-
        Performs synchronous speech recognition: receive results after all audio
        has been sent and processed.
      operationId: speech.speech.recognize
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/RecognizeRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/RecognizeResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - speech
definitions:
  LongRunningRecognizeRequest:
    description: |-
      The top-level message sent by the client for the `LongRunningRecognize`
      method.
    properties:
      audio:
        $ref: '#/definitions/RecognitionAudio'
        description: '*Required* The audio data to be recognized.'
      config:
        $ref: '#/definitions/RecognitionConfig'
        description: |-
          *Required* Provides information to the recognizer that specifies how to
          process the request.
    type: object
  Operation:
    description: |-
      This resource represents a long-running operation that is the result of a
      network API call.
    properties:
      done:
        description: |-
          If the value is `false`, it means the operation is still in progress.
          If `true`, the operation is completed, and either `error` or `response` is
          available.
        type: boolean
      error:
        $ref: '#/definitions/Status'
        description: The error result of the operation in case of failure or cancellation.
      metadata:
        additionalProperties:
          description: Properties of the object. Contains field @type with type URL.
        description: |-
          Service-specific metadata associated with the operation.  It typically
          contains progress information and common metadata such as create time.
          Some services might not provide such metadata.  Any method that returns a
          long-running operation should document the metadata type, if any.
        type: object
      name:
        description: |-
          The server-assigned name, which is only unique within the same service that
          originally returns it. If you use the default HTTP mapping, the
          `name` should have the format of `operations/some/unique/name`.
        type: string
      response:
        additionalProperties:
          description: Properties of the object. Contains field @type with type URL.
        description: |-
          The normal response of the operation in case of success.  If the original
          method returns no data on success, such as `Delete`, the response is
          `google.protobuf.Empty`.  If the original method is standard
          `Get`/`Create`/`Update`, the response should be the resource.  For other
          methods, the response should have the type `XxxResponse`, where `Xxx`
          is the original method name.  For example, if the original method name
          is `TakeSnapshot()`, the inferred response type is
          `TakeSnapshotResponse`.
        type: object
    type: object
  RecognitionAudio:
    description: |-
      Contains audio data in the encoding specified in the `RecognitionConfig`.
      Either `content` or `uri` must be supplied. Supplying both or neither
      returns google.rpc.Code.INVALID_ARGUMENT. See
      [audio limits](https://cloud.google.com/speech/limits#content).
    properties:
      content:
        description: |-
          The audio data bytes encoded as specified in
          `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
          pure binary representation, whereas JSON representations use base64.
        format: byte
        type: string
      uri:
        description: |-
          URI that points to a file that contains audio data bytes as specified in
          `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
          supported, which must be specified in the following format:
          `gs://bucket_name/object_name` (other URI formats return
          google.rpc.Code.INVALID_ARGUMENT). For more information, see
          [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
        type: string
    type: object
  RecognitionConfig:
    description: |-
      Provides information to the recognizer that specifies how to process the
      request.
    properties:
      enableWordTimeOffsets:
        description: |-
          *Optional* If `true`, the top result includes a list of words and
          the start and end time offsets (timestamps) for those words. If
          `false`, no word-level time offset information is returned. The default is
          `false`.
        type: boolean
      encoding:
        description: |-
          Encoding of audio data sent in all `RecognitionAudio` messages.
          This field is optional for `FLAC` and `WAV` audio files and required
          for all other audio formats. For details, see AudioEncoding.
        enum:
          - ENCODING_UNSPECIFIED
          - LINEAR16
          - FLAC
          - MULAW
          - AMR
          - AMR_WB
          - OGG_OPUS
          - SPEEX_WITH_HEADER_BYTE
        type: string
      languageCode:
        description: |-
          *Required* The language of the supplied audio as a
          [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
          Example: "en-US".
          See [Language Support](https://cloud.google.com/speech/docs/languages)
          for a list of the currently supported language codes.
        type: string
      maxAlternatives:
        description: |-
          *Optional* Maximum number of recognition hypotheses to be returned.
          Specifically, the maximum number of `SpeechRecognitionAlternative` messages
          within each `SpeechRecognitionResult`.
          The server may return fewer than `max_alternatives`.
          Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
          one. If omitted, will return a maximum of one.
        format: int32
        type: integer
      profanityFilter:
        description: |-
          *Optional* If set to `true`, the server will attempt to filter out
          profanities, replacing all but the initial character in each filtered word
          with asterisks, e.g. "f***". If set to `false` or omitted, profanities
          won't be filtered out.
        type: boolean
      sampleRateHertz:
        description: |-
          Sample rate in Hertz of the audio data sent in all
          `RecognitionAudio` messages. Valid values are: 8000-48000.
          16000 is optimal. For best results, set the sampling rate of the audio
          source to 16000 Hz. If that's not possible, use the native sample rate of
          the audio source (instead of re-sampling).
          This field is optional for `FLAC` and `WAV` audio files and required
          for all other audio formats. For details, see AudioEncoding.
        format: int32
        type: integer
      speechContexts:
        description: '*Optional* A means to provide context to assist the speech recognition.'
        items:
          $ref: '#/definitions/SpeechContext'
        type: array
    type: object
  RecognizeRequest:
    description: The top-level message sent by the client for the `Recognize` method.
    properties:
      audio:
        $ref: '#/definitions/RecognitionAudio'
        description: '*Required* The audio data to be recognized.'
      config:
        $ref: '#/definitions/RecognitionConfig'
        description: |-
          *Required* Provides information to the recognizer that specifies how to
          process the request.
    type: object
  RecognizeResponse:
    description: |-
      The only message returned to the client by the `Recognize` method. It
      contains the result as zero or more sequential `SpeechRecognitionResult`
      messages.
    properties:
      results:
        description: |-
          Output only. Sequential list of transcription results corresponding to
          sequential portions of audio.
        items:
          $ref: '#/definitions/SpeechRecognitionResult'
        type: array
    type: object
  SpeechContext:
    description: |-
      Provides "hints" to the speech recognizer to favor specific words and phrases
      in the results.
    properties:
      phrases:
        description: |-
          *Optional* A list of strings containing words and phrases "hints" so that
          the speech recognition is more likely to recognize them. This can be used
          to improve the accuracy for specific words and phrases, for example, if
          specific commands are typically spoken by the user. This can also be used
          to add additional words to the vocabulary of the recognizer. See
          [usage limits](https://cloud.google.com/speech/limits#content).
        items:
          type: string
        type: array
    type: object
  SpeechRecognitionAlternative:
    description: Alternative hypotheses (a.k.a. n-best list).
    properties:
      confidence:
        description: |-
          Output only. The confidence estimate between 0.0 and 1.0. A higher number
          indicates an estimated greater likelihood that the recognized words are
          correct. This field is set only for the top alternative of a non-streaming
          result or, of a streaming result where `is_final=true`.
          This field is not guaranteed to be accurate and users should not rely on it
          to be always provided.
          The default of 0.0 is a sentinel value indicating `confidence` was not set.
        format: float
        type: number
      transcript:
        description: Output only. Transcript text representing the words that the user spoke.
        type: string
      words:
        description: |-
          Output only. A list of word-specific information for each recognized word.
          Note: When enable_speaker_diarization is true, you will see all the words
          from the beginning of the audio.
        items:
          $ref: '#/definitions/WordInfo'
        type: array
    type: object
  SpeechRecognitionResult:
    description: A speech recognition result corresponding to a portion of the audio.
    properties:
      alternatives:
        description: |-
          Output only. May contain one or more recognition hypotheses (up to the
          maximum specified in `max_alternatives`).
          These alternatives are ordered in terms of accuracy, with the top (first)
          alternative being the most probable, as ranked by the recognizer.
        items:
          $ref: '#/definitions/SpeechRecognitionAlternative'
        type: array
    type: object
  Status:
    description: |-
      The `Status` type defines a logical error model that is suitable for different
      programming environments, including REST APIs and RPC APIs. It is used by
      [gRPC](https://github.com/grpc). The error model is designed to be:

      - Simple to use and understand for most users
      - Flexible enough to meet unexpected needs

      # Overview

      The `Status` message contains three pieces of data: error code, error message,
      and error details. The error code should be an enum value of
      google.rpc.Code, but it may accept additional error codes if needed.  The
      error message should be a developer-facing English message that helps
      developers *understand* and *resolve* the error. If a localized user-facing
      error message is needed, put the localized message in the error details or
      localize it in the client. The optional error details may contain arbitrary
      information about the error. There is a predefined set of error detail types
      in the package `google.rpc` that can be used for common error conditions.

      # Language mapping

      The `Status` message is the logical representation of the error model, but it
      is not necessarily the actual wire format. When the `Status` message is
      exposed in different client libraries and different wire protocols, it can be
      mapped differently. For example, it will likely be mapped to some exceptions
      in Java, but more likely mapped to some error codes in C.

      # Other uses

      The error model and the `Status` message can be used in a variety of
      environments, either with or without APIs, to provide a
      consistent developer experience across different environments.

      Example uses of this error model include:

      - Partial errors. If a service needs to return partial errors to the client,
          it may embed the `Status` in the normal response to indicate the partial
          errors.

      - Workflow errors. A typical workflow has multiple steps. Each step may
          have a `Status` message for error reporting.

      - Batch operations. If a client uses batch request and batch response, the
          `Status` message should be used directly inside batch response, one for
          each error sub-response.

      - Asynchronous operations. If an API call embeds asynchronous operation
          results in its response, the status of those operations should be
          represented directly using the `Status` message.

      - Logging. If some API errors are stored in logs, the message `Status` could
          be used directly after any stripping needed for security/privacy reasons.
    properties:
      code:
        description: 'The status code, which should be an enum value of google.rpc.Code.'
        format: int32
        type: integer
      details:
        description: |-
          A list of messages that carry the error details.  There is a common set of
          message types for APIs to use.
        items:
          additionalProperties:
            description: Properties of the object. Contains field @type with type URL.
          type: object
        type: array
      message:
        description: |-
          A developer-facing error message, which should be in English. Any
          user-facing error message should be localized and sent in the
          google.rpc.Status.details field, or localized by the client.
        type: string
    type: object
  WordInfo:
    description: Word-specific information for recognized words.
    properties:
      endTime:
        description: |-
          Output only. Time offset relative to the beginning of the audio,
          and corresponding to the end of the spoken word.
          This field is only set if `enable_word_time_offsets=true` and only
          in the top hypothesis.
          This is an experimental feature and the accuracy of the time offset can
          vary.
        format: google-duration
        type: string
      speakerTag:
        description: |-
          Output only. A distinct integer value is assigned for every speaker within
          the audio. This field specifies which one of those speakers was detected to
          have spoken this word. Value ranges from '1' to diarization_speaker_count.
          speaker_tag is set if enable_speaker_diarization = 'true' and only in the
          top alternative.
        format: int32
        type: integer
      startTime:
        description: |-
          Output only. Time offset relative to the beginning of the audio,
          and corresponding to the start of the spoken word.
          This field is only set if `enable_word_time_offsets=true` and only
          in the top hypothesis.
          This is an experimental feature and the accuracy of the time offset can
          vary.
        format: google-duration
        type: string
      word:
        description: Output only. The word corresponding to this set of information.
        type: string
    type: object
